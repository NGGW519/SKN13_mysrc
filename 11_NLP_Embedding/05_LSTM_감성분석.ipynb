{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93c9500-6e1f-445a-a5e1-95e687853ae8",
   "metadata": {},
   "source": [
    "# Pytorch의 nn.Embedding\n",
    "- Pytorch의 Embedding Layer는 word2vec과 마찬가지로 word embedding vector를 찾는 **Lookup Table**이다.\n",
    "    - 단어의 **정수의 고유 index**가 입력으로 들어오면 Embedding Layer의 **그 index의 Vector**를 출력한다.\n",
    "    - 모델이 학습되는 동안 모델이 풀려는 문제에 맞는 값으로 Embedding Layer의 vector들이 업데이트 된다.\n",
    "    - Word2Vec의 embedding vector 학습을 nn.Embedding은 자신이 포함된 모델을 학습 하는 과정에서 한다고 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d936d8f7-a203-4452-978f-e2da81b7dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embed = nn.Embedding(\n",
    "    num_embeddings=20_000,  # vocab size(어휘사전에 있는 단어수(토큰수)) -> 총 몇개의 단어에 대한 embedding vector를 만들지에 대한 설정\n",
    "    embedding_dim=200,      # embedding vector의 차원수 -> 개별단어를 몇개의 숫자(feature)로 표현할지에 대한 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d166e4e-4ced-4bc8-a188-12358670ad0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20000, 200])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.weight\n",
    "# 아래 행렬을 보면, embed.weight[i]는 i번재 단어(token)의 embedding vector값을 나타낸다.\n",
    "# 따라서 20000만개의 embedding vector, 200개의 feature를 가짐\n",
    "# embed.weight.shape >>> 출력: torch.Size([20000, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c2253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 200])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding layer의 입력 - 문서를 구성하는 토큰들의 ID(**정수-int**)를 1차원 묶어서 전달.\n",
    "\n",
    "# 예를들어 doc = 나는-30|어제-159|밥을-9000|먹었다-326\n",
    "doc = torch.tensor([[30,159,9000,326],[30,159,9000,326],[30,159,9000,326]], dtype=torch.int64) # 각 토큰에 대한 id를 지정하여 아래처럼 정보 조회 가능\n",
    "                    # batch_size(문장 개수)가 3이라서 2차원리스트로 리스트 3개를 각각 넣어야한다.\n",
    "embedding_vector = embed(doc)\n",
    "embedding_vector.shape\n",
    "\n",
    "# 출력 결과: torch.Size([1, 4, 200]) >>>  [1:batch_size, 4:seq_len, 200:embedding vector 차원수]\n",
    "# 30, 159, 9000, 326 이라는 id가 200개의 차원을 갖는 vector로 바뀌는 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8210abaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2615, -0.6480,  0.6542,  0.1267, -0.4590, -1.5807, -0.4225,  2.0638,\n",
       "        -0.0388,  1.4628,  1.0835, -0.6870, -0.1393,  0.0141, -3.1681,  0.7286,\n",
       "         0.3485, -1.6117,  1.0816,  0.3718,  1.2983,  0.6510,  0.5335,  0.9103,\n",
       "        -0.6888,  0.5167, -0.6261, -0.3301, -0.3349,  0.1504, -0.6173, -0.3165,\n",
       "        -0.6558, -0.8535,  1.9076,  0.0057,  0.1901, -0.3131,  1.3714, -0.0869,\n",
       "         0.6757, -0.2383,  0.6748,  1.3897, -0.0878,  0.6294, -0.5268, -1.2177,\n",
       "         0.2837,  0.4953, -1.5418, -0.4043,  0.7954,  1.6025, -2.2578, -0.4963,\n",
       "        -0.8050,  0.5768, -0.9294, -1.0111, -0.1835,  1.0086, -0.5453, -0.4286,\n",
       "        -0.3882,  1.1476, -1.0053,  1.6031,  1.2461, -0.3062, -1.2062, -2.8213,\n",
       "        -1.1075, -0.5217,  0.2048, -0.1997,  1.7862,  0.1536,  1.6618,  0.1133,\n",
       "        -0.1169,  0.7271,  0.8280,  2.5958,  1.0975,  0.1266, -0.1898,  0.3918,\n",
       "         0.2049,  0.3505,  0.9007,  1.6800,  0.0518, -0.3274, -0.2015,  0.3683,\n",
       "        -0.1802, -0.1015,  0.0472,  1.4299,  2.1903, -0.8712,  0.0970, -0.9406,\n",
       "         0.8492,  1.5976,  0.5136,  1.5880,  0.6544,  0.3315,  1.4310, -0.7426,\n",
       "         2.0310, -0.0095,  0.6344, -1.2820, -0.1719, -0.1003,  0.1120,  0.5705,\n",
       "        -0.8667, -0.0484, -0.2008, -0.2347,  0.4048, -0.5157,  3.2819, -1.2849,\n",
       "         0.7035, -0.0582, -1.4262, -1.2471, -0.8687,  0.1329, -0.8047,  0.1590,\n",
       "        -0.6043,  0.8644, -0.8560, -0.4001, -0.2006, -1.2022,  1.7195,  0.1382,\n",
       "         0.2696,  0.3727, -0.5889, -1.9677, -0.6220,  0.1988, -0.1532, -1.3237,\n",
       "         0.2895, -0.1258, -1.3119,  0.9110,  0.1041, -0.8678,  0.7913,  1.6650,\n",
       "        -0.3066, -1.1589, -0.6972,  0.6780,  0.3401, -2.9907,  0.7362, -0.9732,\n",
       "        -1.4159, -1.1411,  1.0782,  0.6087, -0.3391,  2.0366,  1.0192,  0.6224,\n",
       "         1.0033, -1.1222,  0.0537,  0.1956, -0.4978,  0.7570,  1.4287, -1.2958,\n",
       "        -0.5413,  0.7787, -0.6163, -0.2096, -0.1444, -0.5403, -1.0728, -0.5669,\n",
       "        -1.4810, -0.5156,  0.4794, -0.1554, -0.4035, -0.6682, -0.0199,  0.9740],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8314e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2615, -0.6480,  0.6542,  0.1267, -0.4590, -1.5807, -0.4225,  2.0638,\n",
       "        -0.0388,  1.4628,  1.0835, -0.6870, -0.1393,  0.0141, -3.1681,  0.7286,\n",
       "         0.3485, -1.6117,  1.0816,  0.3718,  1.2983,  0.6510,  0.5335,  0.9103,\n",
       "        -0.6888,  0.5167, -0.6261, -0.3301, -0.3349,  0.1504, -0.6173, -0.3165,\n",
       "        -0.6558, -0.8535,  1.9076,  0.0057,  0.1901, -0.3131,  1.3714, -0.0869,\n",
       "         0.6757, -0.2383,  0.6748,  1.3897, -0.0878,  0.6294, -0.5268, -1.2177,\n",
       "         0.2837,  0.4953, -1.5418, -0.4043,  0.7954,  1.6025, -2.2578, -0.4963,\n",
       "        -0.8050,  0.5768, -0.9294, -1.0111, -0.1835,  1.0086, -0.5453, -0.4286,\n",
       "        -0.3882,  1.1476, -1.0053,  1.6031,  1.2461, -0.3062, -1.2062, -2.8213,\n",
       "        -1.1075, -0.5217,  0.2048, -0.1997,  1.7862,  0.1536,  1.6618,  0.1133,\n",
       "        -0.1169,  0.7271,  0.8280,  2.5958,  1.0975,  0.1266, -0.1898,  0.3918,\n",
       "         0.2049,  0.3505,  0.9007,  1.6800,  0.0518, -0.3274, -0.2015,  0.3683,\n",
       "        -0.1802, -0.1015,  0.0472,  1.4299,  2.1903, -0.8712,  0.0970, -0.9406,\n",
       "         0.8492,  1.5976,  0.5136,  1.5880,  0.6544,  0.3315,  1.4310, -0.7426,\n",
       "         2.0310, -0.0095,  0.6344, -1.2820, -0.1719, -0.1003,  0.1120,  0.5705,\n",
       "        -0.8667, -0.0484, -0.2008, -0.2347,  0.4048, -0.5157,  3.2819, -1.2849,\n",
       "         0.7035, -0.0582, -1.4262, -1.2471, -0.8687,  0.1329, -0.8047,  0.1590,\n",
       "        -0.6043,  0.8644, -0.8560, -0.4001, -0.2006, -1.2022,  1.7195,  0.1382,\n",
       "         0.2696,  0.3727, -0.5889, -1.9677, -0.6220,  0.1988, -0.1532, -1.3237,\n",
       "         0.2895, -0.1258, -1.3119,  0.9110,  0.1041, -0.8678,  0.7913,  1.6650,\n",
       "        -0.3066, -1.1589, -0.6972,  0.6780,  0.3401, -2.9907,  0.7362, -0.9732,\n",
       "        -1.4159, -1.1411,  1.0782,  0.6087, -0.3391,  2.0366,  1.0192,  0.6224,\n",
       "         1.0033, -1.1222,  0.0537,  0.1956, -0.4978,  0.7570,  1.4287, -1.2958,\n",
       "        -0.5413,  0.7787, -0.6163, -0.2096, -0.1444, -0.5403, -1.0728, -0.5669,\n",
       "        -1.4810, -0.5156,  0.4794, -0.1554, -0.4035, -0.6682, -0.0199,  0.9740],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.weight[30] # 위와 결과 같음\n",
    "# weight가 embedding_vector 값이다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc67156",
   "metadata": {},
   "source": [
    "num_embeddings=20_000, embedding_dim=200 이므로,\n",
    "단어수는 20000, 하나의 embedding vector를 이루는 값들은 200개\n",
    "weight에 입력값을 넣을 때, 입력값은 ohe 된것(ex. [0,0,1,0,0,0])이라서 weight중 1row만 반영된다.\n",
    "그렇게 학습이 된 후에, 예측하려는 문장을 토큰화한 후에 나온 숫자들은 각각 ohe되어 반영되었던 row와 idx가 같다.\n",
    "따라서 embeddig vector에 반영되는 idx에 해당하는 weight의 row만 따로 모아보면, 양자는 같다.\n",
    "\n",
    "즉, 어휘사전에 단어의 id는, 이후에 입력되는 문장이 토큰화 되어 결정되는 id와 같다.\n",
    "아래는 그 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b62477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 어휘사전 어휘 수 : 10개\n",
    "# 임베딩 벡터 차원수 : 3차원 (1개의 단어에 대해서 3개의 값을 갖겠다.)\n",
    "\n",
    "e_layer = nn.Embedding(\n",
    "    num_embeddings=10,\n",
    "    embedding_dim=3,\n",
    ")\n",
    "# 위에서 만든 embedding layer가 결국은 10X3 weight 행렬을 생성한 것이다. -> weight 행렬이 전체 어휘들의 embedding vectoer들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16f652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0258,  0.2443,  0.4804],\n",
       "        [ 0.9972,  0.3877, -0.0814],\n",
       "        [-0.0971,  0.4244,  0.0437],\n",
       "        [ 0.2652, -1.4574, -1.0347],\n",
       "        [ 1.5640, -0.5004,  0.5605],\n",
       "        [-0.1732,  0.6381, -0.6108],\n",
       "        [-0.5190,  0.5237, -1.5154],\n",
       "        [ 0.1994,  1.1014,  0.8350],\n",
       "        [-1.4112, -0.9116,  1.6399],\n",
       "        [-0.4163,  0.2410,  1.7471]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_layer.weight\n",
    "# weight의 row 한줄한줄이 어휘사전의 어휘에 대한 weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "306b00f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0258,  0.2443,  0.4804],\n",
       "        [ 1.5640, -0.5004,  0.5605],\n",
       "        [-0.0971,  0.4244,  0.0437]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예를 들어 \"오늘 날씨 좋다\"의 문장을 입력할건데, 각 어휘의 토큰이 오늘:0, 날씨:4, 좋다:2 라면?\n",
    "sent = \"오늘 날씨 좋다\"\n",
    "# token = tokenizer.encode(sent).ids >> 이걸통해 문장을 토큰화하고 단어의 id를 찾아낸다.\n",
    "token = torch.tensor([0,4,2], dtype=torch.int64)\n",
    "e_layer(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef91d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 아래의 vector는 embedding layer를 다시 돌리면 값이 달라진다.\n",
    "[\n",
    "    [ 0.0258,  0.2443,  0.4804], <- 0:오늘\n",
    "    [ 1.5640, -0.5004,  0.5605], <- 4:날씨\n",
    "    [-0.0971,  0.4244,  0.0437]  <- 2:좋다\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcee9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca59e731-381d-4a79-83c4-1fc20ba006e1",
   "metadata": {},
   "source": [
    "# 네이버 영화 댓글 감성분석(Sentiment Analysis)\n",
    "\n",
    "## 감성분석(Sentiment Analysis) 이란\n",
    "입력된 텍스트가 **긍적적인 글**인지 **부정적인**인지 또는 **중립적인** 글인지 분석하는 것을 감성(감정) 분석이라고 한다.   \n",
    "이를 통해 기업이 고객이 자신들의 기업 또는 제품에 대해 어떤 의견을 가지고 있는지 분석한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7034ada-08b9-4163-b18d-ce429aef275b",
   "metadata": {},
   "source": [
    "# Dataset, DataLoader 생성\n",
    "\n",
    "## Korpora에서 Naver 영화 댓글 dataset 가져오기\n",
    "- https://ko-nlp.github.io/Korpora/ko-docs/corpuslist/nsmc.html\n",
    "- http://github.com/e9t/nsmc/\n",
    "    - input: 영화댓글\n",
    "    - output: 0(부정적댓글), 1(긍정적댓글)\n",
    "### API\n",
    "- **corpus 가져오기**\n",
    "    - `Korpora.load('nsmc')`\n",
    "- **text/label 조회**\n",
    "    - `corpus.get_all_texts()` : 전체 corpus의 text들을 tuple로 반환\n",
    "    - `corpus.get_all_labels()`: 전체 corpus의 label들을 list로 반환\n",
    "- **train/test set 나눠서 조회**\n",
    "    - `corpus.train`\n",
    "    - `corpus.test`\n",
    "    - `LabeledSentenceKorpusData` 객체에 text와 label들을 담아서 제공.\n",
    "        - `LabeledSentenceKorpusData.texts`: text들 tuple로 반환.\n",
    "        - `LabeledSentenceKorpusData.labels`: label들 list로 반환."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e2ea3-6123-4ebd-8e98-27b1db6406ed",
   "metadata": {},
   "source": [
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aebdb0ac-eaaa-47aa-a0de-11d49e8a427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\Playdata\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\Playdata\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from Korpora import Korpora\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08b08d5-bfcd-4430-b4c0-44b19bbf4022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('아 더빙.. 진짜 짜증나네요 목소리',\n",
       " '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나',\n",
       " '너무재밓었다그래서보는것을추천한다',\n",
       " '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정',\n",
       " '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_inputs = corpus.get_all_texts() # inputs: 댓글들 전체\n",
    "all_labels = corpus.get_all_labels() # outputs: labels 전체 - 0: 부정, 1:긍정\n",
    "all_inputs[:5] #0~4번까지 댓글 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232a23c0-06c5-49b7-b601-1ede1198b4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels[:5]\n",
    "\n",
    "# 출력결과: [0, 1, 0, 0, 1] >> 1번과 4번은 긍정적인 댓글로 파악한것\n",
    "# 위 데이터는 평점을 반영한 데이터라서, '너무재밓었다그래서보는것을추천한다' >> 긍정댓글인듯 보이지만, 평점을 낮게 줘서 '0'이라서 나온것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8899b607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NSMC.train: size=150000\n",
       "  - NSMC.train.texts : list[str]\n",
       "  - NSMC.train.labels : list[int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e89634f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NSMC.test: size=50000\n",
       "  - NSMC.test.texts : list[str]\n",
       "  - NSMC.test.labels : list[int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9df89ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('굳 ㅋ',\n",
       " 'GDNTOPCLASSINTHECLUB',\n",
       " '뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아',\n",
       " '지루하지는 않은데 완전 막장임... 돈주고 보기에는....',\n",
       " '3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test.texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b8e668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test.labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab492d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2e7e54a-548d-4bf3-81aa-357ab249f41a",
   "metadata": {},
   "source": [
    "## 토큰화\n",
    "1. 형태소 단위 token화(분절)를 먼저 한다.\n",
    "    - konlpy로 token화 한 뒤 다시 한 문장으로 만든다.\n",
    "2. 1에서 처리한 corpus를 BPE 로 token화\n",
    "   \n",
    "### 전처리 함수\n",
    "\n",
    "#### 형태소 단위 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0bbb39b-9f49-4d29-a969-4839c01f430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import string\n",
    "import re\n",
    "\n",
    "okt= Okt()\n",
    "# 전처리 = cleansing + 정규화(normalize)\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    1. 영문 -> 소문자로 변환\n",
    "    2. 구두점 제거\n",
    "    3. 형태소 기반 토큰화\n",
    "    4. 형태소로 토큰화 한 뒤 다시 하나의 문자열로 묶어서 반환.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # 구두점 제거(stop word(불용어))\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
    "    # 정규화\n",
    "    tokens = okt.morphs(text, stem=True) # stem: 원형복원.\n",
    "    return ' '.join(tokens) # list인 [\"단어\", \"단어\"....] -> str로 변환 \"단어 단어 단어\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35f45e2f-f76a-4011-b963-d7feb214cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "좀 검증된 애들좀 출현시켜라이탈리아 특집 장난하냐\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'좀 검증 되다 애 들 좀 출현 시키다 이탈리아 특집 장난 하다'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_inputs[400])\n",
    "text_preprocessing(all_inputs[400])   # 토큰화 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e658962-2fd6-4b1d-b4ef-a38de3ecc847",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# train set 전처리\u001b[39;00m\n\u001b[32m      4\u001b[39m train_texts = corpus.train.texts\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m train_inputs = [\u001b[43mtext_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m txt \u001b[38;5;129;01min\u001b[39;00m train_texts]\n\u001b[32m      6\u001b[39m train_labels = corpus.train.labels\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# test set 전처리\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtext_preprocessing\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     16\u001b[39m text = re.sub(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstring.punctuation\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, text)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 정규화\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m tokens = \u001b[43mokt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmorphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# stem: 원형복원.\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\AppData\\Local\\miniconda3\\envs\\dl\\Lib\\site-packages\\konlpy\\tag\\_okt.py:89\u001b[39m, in \u001b[36mOkt.morphs\u001b[39m\u001b[34m(self, phrase, norm, stem)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmorphs\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase, norm=\u001b[38;5;28;01mFalse\u001b[39;00m, stem=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     87\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\AppData\\Local\\miniconda3\\envs\\dl\\Lib\\site-packages\\konlpy\\tag\\_okt.py:71\u001b[39m, in \u001b[36mOkt.pos\u001b[39m\u001b[34m(self, phrase, norm, stem, join)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[33;03m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     69\u001b[39m validate_phrase_inputs(phrase)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjki\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjava\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjava\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.toArray()\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m join:\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "# train set 전처리\n",
    "train_texts = corpus.train.texts\n",
    "train_inputs = [text_preprocessing(txt) for txt in train_texts]\n",
    "train_labels = corpus.train.labels\n",
    "\n",
    "# test set 전처리\n",
    "test_texts = corpus.test.texts\n",
    "test_inputs = [text_preprocessing(txt) for txt in test_texts]\n",
    "test_labels = corpus.test.labels\n",
    "e = time.time()\n",
    "\n",
    "print('전처리 걸린시간(초):', e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a25c93-7e26-4512-a0c0-56218fcda100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "os.makedirs(\"datasets/nsmc\", exist_ok=True)\n",
    "with open(\"datasets/nsmc/preprocessing_trainset.pkl\", \"wb\") as fw:\n",
    "    pickle.dump({\"input\":train_inputs, \"output\":train_labels}, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7cad7-23d9-4212-b07a-0e5da2ff73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/nsmc/preprocessing_testset.pkl\", \"wb\") as fw:\n",
    "    pickle.dump({\"input\":test_inputs, \"output\":test_labels}, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb8f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle로 저장한 전처리된 데이터셋 읽어오기.\n",
    "import pickle\n",
    "\n",
    "with open(\"datasets/nsmc/preprocessing_trainset.pkl\", \"rb\") as fr:\n",
    "    train_dict = pickle.load(fr)\n",
    "\n",
    "with open(\"datasets/nsmc/preprocessing_testset.pkl\", \"rb\") as fr:\n",
    "    test_dict = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_dict['input']\n",
    "train_labels = train_dict['output']\n",
    "\n",
    "test_inputs = test_dict['input']\n",
    "test_labels = test_dict['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc935fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = train_inputs + test_inputs # vocab 만들때 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a7213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000, 200000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs), len(test_inputs), len(all_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기까지는 전처리, 이하 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e519a68-d3a0-4481-bcf7-b121d8ba813f",
   "metadata": {},
   "source": [
    "### 토큰화\n",
    "- Subword 방식 토큰화 적용\n",
    "- Byte Pair Encoding 방식으로 huggingface tokenizer 사용\n",
    "    - BPE: 토큰을 글자 단위로 나눈뒤 가장 자주 등장하는 글자 쌍(byte paire)를 찾아 합친뒤 어휘사전에 추가한다.\n",
    "    - https://huggingface.co/docs/tokenizers/quicktour\n",
    "    - `pip install tokenizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f162bdf-fac9-4468-a264-c656e4b3164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE #, Unigram, WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocabs_size = 30_000  # 어휘사전의 최대 단어수\n",
    "min_frequency = 5 # 어휘를 다 넣지 않고, 최소 5번은 나와야 넣겠다는 것\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    BPE(unk_token=\"[UNK]\")\n",
    ")\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocabs_size,\n",
    "    min_frequency=min_frequency,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\"],\n",
    "    continuing_subword_prefix=\"##\"\n",
    "    # 단어 중간에 나오는 subword일 경우 앞에 ##을 붙인다.\n",
    "    # \"시작하는\" -> \"시작\", \"하는\" => \"시작\", \"##하는\"\n",
    ")\n",
    "tokenizer.train_from_iterator(all_inputs, trainer=trainer) #vocab 생성 == tokenizer 학습.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b29e1-384a-44e8-ab19-e53f0c1303c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26739"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 vocab size:\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eda82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p',\n",
       " '##y',\n",
       " '##t',\n",
       " '##or',\n",
       " '##ch',\n",
       " '##와',\n",
       " 'p',\n",
       " '##and',\n",
       " '##as',\n",
       " '##와',\n",
       " 'n',\n",
       " '##um',\n",
       " '##p',\n",
       " '##y',\n",
       " '##는',\n",
       " 'p',\n",
       " '##y',\n",
       " '##th',\n",
       " '##on',\n",
       " '##의',\n",
       " '라이브',\n",
       " '##러리',\n",
       " '입',\n",
       " '##니다',\n",
       " '[UNK]']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = tokenizer.encode(\"pytorch와 pandas와 numpy는 python의 라이브러리 입니다.\")\n",
    "e.tokens\n",
    "\n",
    "# 마침표는 어휘사전에 없다. 그래서 [UNK]라고 나옴\n",
    "# 위의 영단어는 영화 댓글에 대한 어휘사전에 있을리 없어서, 단어가 쪼개져서 토큰화됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ddd5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p ##y ##t ##or ##ch ##와 p ##and ##as ##와 n ##um ##p ##y ##는 p ##y ##th ##on ##의 라이브 ##러리 입 ##니다'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰을 옆으로 붙여보기\n",
    "tokenizer.decode(e.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998d38b-e762-4fd5-b37f-8f8a2b6f5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "os.makedirs(\"saved_models/nsmc\", exist_ok=True)\n",
    "tokenizer.save(\"saved_models/nsmc/tokenizer_bpe.json\")\n",
    "\n",
    "# 불러올땐?\n",
    "# load_tokenizer = Tokenizer.from_file(\"saved_models/nsmc/tokenizer_bpe.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81597ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정말 최고 의 명작 성인 이 되다 보다 이집트 의 왕자 는 또 다른 감동 그 자체 네 요\n",
      "[5420, 5438, 2203, 5530, 6570, 2206, 5425, 5410, 14757, 2203, 9123, 923, 1152, 5617, 5450, 651, 5641, 856, 2128]\n",
      "['정말', '최고', '의', '명작', '성인', '이', '되다', '보다', '이집트', '의', '왕자', '는', '또', '다른', '감동', '그', '자체', '네', '요']\n"
     ]
    }
   ],
   "source": [
    "# 토큰화되어 id가 매칭되는 것 확인. 1000번은 예시를 든것\n",
    "\n",
    "idx = 1000\n",
    "print(all_inputs[idx])\n",
    "tokens = tokenizer.encode(all_inputs[idx])\n",
    "print(tokens.ids)\n",
    "print(tokens.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf797f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'정말 최고 의 명작 성인 이 되다 보다 이집트 의 왕자 는 또 다른 감동 그 자체 네 요'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5c31d-633c-4a31-8f66-dff2ecf8e86a",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2441f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch 사용자 정의 Dataset(Custom Dataset) 정의\n",
    "# 1. Dataset 상속\n",
    "# 2. __len__(self) : 총 데이터의 개수 반환 -> len()와 연동\n",
    "# 3. __getitem__(self, index) : index의 x,y를 반환.- 객체[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff27280e-dfb7-4947-9192-777e6984286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_length, tokenizer):\n",
    "        \"\"\"\n",
    "        texts: list - 댓글 목록. 리스트에 댓글들을 담아서 받는다. [\"댓글\", \"댓글\", ...]\n",
    "        labels: list - 댓글 감정(labels의 긍/부정) 목록. \n",
    "        max_length: 개별 댓글의 token 개수. 모든 댓글의 토큰수를 max_length에 맞춘다. 댓글마다 길이가 달라서 sequence의 length(개수)를 맞춰주기 위함.\n",
    "        tokenizer: Tokenizer\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        self.texts = [self.__pad_token_sequences(tokenizer.encode(txt).ids) for txt in texts] # 한문장 한문장을 토크나이저하여 그 id로 이루어진 것으로 변환하여 하나씩 보낸다.\n",
    "        # self.texts: 입력댓글 - token is로 변환된 댓글(문서). 글자수는 max_length에 맞춤.\n",
    "        #             max_length 보다 적으면 [PAD] 추가, max_length보다 많으면 잘라낸다.\n",
    "        #             아래의 __pad_token_sequences()가 그 역할을 한다.\n",
    "\n",
    "    ############################################################################################\n",
    "    # id로 구성된 개별 문장 token list를 받아서 패딩 추가 [20, 2, 1] => [20, 2, 1, 0, 0, 0, ..]\n",
    "    # max_length에 토큰리스트의 개수를 맞춰주는 함수.\n",
    "    ############################################################################################\n",
    "    def __pad_token_sequences(self, token_sequences): # 함수명 앞에만 '__'붙인 경우, class 안에서만 쓴다는 것(은닉성). 외부에서 호출 불가.\n",
    "                                                      # 뒤에도 '__' 붙이면 앞에 붙인 은닉성이 해제됨\n",
    "        \"\"\"\n",
    "        id로 구성된 개별 문서(댓글)의 token_id list를 받아서 max_length 길이에 맞추는 메소드\n",
    "        max_length 보다 토큰수가 적으면 [PAD] 추가, 많으면 max_length 크기로 줄인다.\n",
    "            ex) if)max_length = 5, [PAD] token id = 0 이라면?\n",
    "            >>>>> [20, 2, 1] => [20, 2, 1, 0, 0, 0, ..]\n",
    "            >>>>> [20, 30, 40, 50, 60, 70, 80] -> [20, 30, 40, 50, 60]\n",
    "            \n",
    "        \"\"\"\n",
    "        pad_token_id = self.tokenizer.token_to_id(\"[PAD]\")\n",
    "        seq_len = len(token_sequences) # 입력받은 토큰 개수\n",
    "        result = None\n",
    "        if seq_len > self.max_length: # 잘라내기\n",
    "            result = token_sequences[:self.max_length]\n",
    "        else:\n",
    "            result = token_sequences + ([pad_token_id] * (self.max_length - seq_len))\n",
    "            #[PAD]는 리스트이므로 [PAD]에 숫자를 곱하면 그만큼 더해진다.\n",
    "        return result\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels) # 총 데이터개수를 반환.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        idx 번째 text와 label을 학습 가능한 type으로 변환해서 반환\n",
    "        Parameter\n",
    "            idx: int 조회할 index\n",
    "        Return\n",
    "            tuple: (torch.LongTensor, torch.FloatTensor) - 댓글 토큰_id 리스트, 정답 Label\n",
    "        \"\"\"\n",
    "        txt = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return (torch.tensor(txt, dtype=torch.int64), torch.tensor([label], dtype=torch.float32))\n",
    "        # [label]: 리스트화 한 이유. 정답과 모델이 예측한 값의 shape을 맞추기 위함. 안하면 y가 그냥 1차원이 된다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515bb6f-703d-4277-b645-8869267f5297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 11, 10, 9, 22]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_length = 5라고 두었지만, 5는 적절한 값일까?\n",
    "# 댓글의 길이가 어떻게 분포되었는지도 모르는데, 5라고 임의로 적용한건 적절치 않다.\n",
    "# 따라서 전체 댓글들의 토큰 수를 확인해보자!\n",
    "# 모든 댓글의 토큰 수 조회\n",
    "all_input_length = [len(tokenizer.encode(txt)) for txt in all_inputs]\n",
    "all_input_length[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f63e1-4ba8-4ccc-8c62-422b1f30b431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 89)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.min(all_input_length), np.max(all_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee963d58-0008-4fa6-b426-e3e332591f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29., 41.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(all_input_length, q=[0.9, 0.95])\n",
    "# 전체 중 90%의 토큰수는 29 미만, 95%는 41개 미만\n",
    "# 95%까진 반영하고 싶으면 max_length = 41로 두면된다. 아래는 max_length = 30으로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eea5f00e-70fa-475b-8e2d-d06d120e3bdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m MAX_LENGTH = \u001b[32m30\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m trainset = NSMCDataset(\u001b[43mtrain_inputs\u001b[49m, train_labels, MAX_LENGTH, tokenizer)\n\u001b[32m      3\u001b[39m testset = NSMCDataset(test_inputs, test_labels, MAX_LENGTH, tokenizer)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 30\n",
    "trainset = NSMCDataset(train_inputs, train_labels, MAX_LENGTH, tokenizer)\n",
    "testset = NSMCDataset(test_inputs, test_labels, MAX_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf0d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc7532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1986, 5881, 5426, 5667, 6087,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0]),\n",
       " tensor(0.))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0] # 30개 맞춘것 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db59e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ebe07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2343, 782)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5f038-b32c-4e4e-82c8-956c7cbe0c4d",
   "metadata": {},
   "source": [
    "# 모델링\n",
    "- Embedding Layer를 이용해 Word Embedding Vector를 추출한다.\n",
    "- LSTM을 이용해 Feature 추출\n",
    "- Linear + Sigmoid로 댓글 긍정일 확률 출력\n",
    "  \n",
    "![outline](figures/rnn/RNN_outline.png)\n",
    "\n",
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88afd6-5c8f-4ade-b930-86c9425e86e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e9cbbd8-d8b4-4a51-ac7a-5765722f139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class NSMCClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabs_size, embedding_dim, hidden_size, num_layers, bidirectional=True, dropout_rate=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size(int) - 어휘사전의 총 어휘수\n",
    "            embedding_dim(int) - (word) embedding vector의 차원수\n",
    "            hideen_size(int) - LSTM의 hidden state의 feature수\n",
    "            num_layers(int) - LSTM의 later의 개수\n",
    "            bidirectional(bool) - LSTM의 양방향 여부\n",
    "            dropout_rate(float) - LSTM이 두개 이상의 layer로 구성된 경우 적용할 dropout 비율. Dropout Layer의 dropout 비율\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 모델을 구성할 Layer들을 정의 - Embedding, LSTM, Dropout, Linear(추론기기), Sigmoid\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocabs_size,  # 총 단어수 -> tokenizer에 등록된 총 단어수.\n",
    "            embedding_dim=embedding_dim, # 임베딩 벡터의 차원수\n",
    "            padding_idx=0 # [PAD]의 토큰ID\n",
    "                          # 원래는 >>> tokenizer.token_to_index(\"[PAD]\")<<< 이렇게 넣어야하는데, 지금 우린 [PAD]=0인걸 알아서 그냥 0으로 넣음\n",
    "                          # padding 토큰은 학습하지 않는다.\n",
    "        )\n",
    "        # Embedding layer의 출력 shape: (batch_size:64, seq_length: 문서 토큰수(30으로 맞췄으니 30이 될것), embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, # 개별 토큰(단어)의 feature수(embedding에서 LSTM으로 넘어가므로 embedding 차원수를 넣는다.)\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0  # LSTM 모델에 layer가 쌓이는 경우에만(stacked rnn) dropout을 적용한다.\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate) # LSTM 과 Linear 사이에 과적합 방지를 위해서 사용.\n",
    "\n",
    "        input_features = hidden_size*2 if bidirectional else hidden_size\n",
    "        self.classifier = nn.Linear(input_features, 1) # 출력 1: 이진분류 -> positive의 확률\n",
    "        # 위의 nn.Linear의 입력값 대한 설명\n",
    "        # 상기 그림을 보면, Feature Extractor 단계에서 보면 몇단계든 간에 마지막 hidden state의 출력값이 Linear의 입력값이 된다.\n",
    "        # 그러나, 양방향인 경우 hidden state의 출력이 2개가 된다.\n",
    "        # 따라서 단방향이면 2배로해서 넣고, 양방향이면 이미 2배이므로 그대로 넣는다.\n",
    "        # 한편 lstm의 출력 형태는 아래와 같다.\n",
    "        # lstm의 출력: out, (hidden, cell)\n",
    "        # out: 모든 timestep의 hidden state값 - [seq_len, batch, hidden * bidirectional]\n",
    "        # hidden: 마지막 timestep의 hidden state(단기기억) - [bidirectional * num_layers, batch, hidden] >>> 순/역방향 2개 >>> layer별로 2개\n",
    "        # cell: 마지막 timestep의 cell state(장기기억) >>> 순/역방향 2개 >>> layer별로 2개\n",
    "        # out을 이용하는 것이 더 편하다.\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid() # classifier의 출력값을 확률(0~1)값으로 변환하는 함수\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X(tensor): 입력 문서의 토큰 리스트. shape: [batch_size, seq_length(max_length:문서구성토큰수)] >>> [64, 30]\n",
    "        \"\"\"\n",
    "        embedding_vectors = self.embedding(X)\n",
    "        # 1) [batch, seq_len] -> embedding layers를 거치면 -> [batch, seq_len, embedding_차원수] \n",
    "        # 2) LSTM -batch)first=False: 입력 shape - [seq_len, batch_size, embedding_dim]\n",
    "        # 1)과 2)를 비교해서 seq_len과 batch 자리가 반대이므로 축(값의 위치)을 바꿔줘야한다.\n",
    "        embedding_vectors = embedding_vectors.transpose(1,0)\n",
    "        out, _ = self.lstm(embedding_vectors)\n",
    "        # out.shape : [seq_len, batch, hidden_size*(2 if bidirectional else 1)] >>> 여기서 seq_len만 쓴다는 것\n",
    "        # classifier(linear)에는 ont의 마지막 index(마지막 seq) 값을 입력\n",
    "\n",
    "        output = self.dropout(out[-1])\n",
    "        output = self.classifier(output) # 윗 줄의 dropout을 입력\n",
    "        last_output = self.sigmoid(output) # 윗 줄의 결과를 확률값으로 변환\n",
    "        return last_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transpose(1,0)???\n",
    "# # 파라미터의 순서: index를 이동시킬 축 위치\n",
    "# # 파라미터 값 : 이동할 대상 index의 축 위치\n",
    "# # 1은 0으로, 0은 1로\n",
    "\n",
    "# # 예시\n",
    "# ev = [\n",
    "#     [10,20,30],\n",
    "#     [40,50,60]\n",
    "# ] # shape: (2,3)\n",
    "\n",
    "# a = ev.transpose(1,0)\n",
    "# \"\"\"\n",
    "# 10 idx: [0,0] >>> [0,0]\n",
    "# 20 idx: [0,1] >>> [1,0]\n",
    "# 30 idx: [0,2] >>> [2,0]\n",
    "# 40 idx: [1,0] >>> [0,1]\n",
    "# 50 idx: [2,0] >>> [0,2]\n",
    "# 60 idx: [3,0] >>> [0,3]\n",
    "# \"\"\"\n",
    "\n",
    "# # 결과\n",
    "# a = [\n",
    "#     [10, 40],\n",
    "#     [20, 50],\n",
    "#     [30, 60]\n",
    "# ]\n",
    "\n",
    "# # reshape과 비교\n",
    "# b = ev.reshape(3,2) = [\n",
    "#     [10,20],\n",
    "#     [30,40],\n",
    "#     [50,60]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a0171-371e-4cb5-9bd5-ad1e3c14480d",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830d2b5-d5ed-4b53-a442-bebc83077aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSMCClassifier(\n",
      "  (embedding): Embedding(26739, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 64, num_layers=2, dropout=0.3, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (classifier): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.get_vocab_size() # 총 어휘수\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "# 모델의 복잡도를 올리려면? >>> EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS를 크게\n",
    "# Auto regressive 모델이 아니면 BIDIRECTIONAL=True(양방향)\n",
    "\n",
    "model = NSMCClassifier(\n",
    "    vocabs_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04a25a-c6dc-4833-883b-54972a16c171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NSMCClassifier                           [64, 1]                   --\n",
       "├─Embedding: 1-1                         [64, 30, 100]             2,673,900\n",
       "├─LSTM: 1-2                              [30, 64, 128]             184,320\n",
       "├─Dropout: 1-3                           [64, 128]                 --\n",
       "├─Linear: 1-4                            [64, 1]                   129\n",
       "├─Sigmoid: 1-5                           [64, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 2,858,349\n",
       "Trainable params: 2,858,349\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 525.03\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 3.50\n",
       "Params size (MB): 11.43\n",
       "Estimated Total Size (MB): 14.95\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary\n",
    "i = torch.randint(1, 10, (64, MAX_LENGTH)) # int64 타입의 dummy 입력데이터\n",
    "# 입력 shape: (batch, seq_len)\n",
    "summary(model, input_data=i, device=device)\n",
    "# summary(모델, input_shape) => 내부적으로 입력데이터(float32)를 생성해서 추론\n",
    "# 아래에서 LSTM 의 축이 바뀐것도 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd5885-8150-4529-ad3a-84931a8824c5",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a1bf6-d8eb-42d0-996e-f975e93888af",
   "metadata": {},
   "source": [
    "### Train/Test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46099bec-eee3-4cef-921b-ce9ee6cf0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch train 하는 함수\n",
    "def train(model, dataloader, loss_fn, optimizer, device=\"cpu\"):\n",
    "    model.train()               # 모델을 train 모드로 변환\n",
    "    model = model.to(device)    # 모델을 device로 이동\n",
    "    total_loss = 0.0            # step별 loss 누적\n",
    "    for X,y in dataloader:      # step 단위로 모델 학습 (batch)\n",
    "        X,y = X.to(device), y.to(device)        # 1. X,y를 device로 이동 (항상 X,y,model은 같은 디바이스에 있어야한다.)\n",
    "        pred = model(X)                         # 2. 추론\n",
    "        loss = loss_fn(pred, y)                 # 3. loss 계산 (여기까지가 순전파)\n",
    "        loss.backward()                         # 4. gradient 계산\n",
    "        optimizer.step()                        # 5. 파라미터 업데이트 (w.data - w.grad *lr)\n",
    "        optimizer.zero_grad()                   # 6. gradient 초기화\n",
    "        total_loss += loss.item()               # 7. loss 누적\n",
    "    # 1 epoch 학습 완료\n",
    "    return total_loss / len(dataloader) # 1 epoch의 train loss를 반환. (toal loss/step수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5eb3ea-78e8-4248-a8ce-d07a4c362d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch 평가/검증 함수\n",
    "def test(model, dataloader, loss_fn, device=\"cpu\"):\n",
    "    # 1. 모델을 eval 모드로 변경, model의 device 이동\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    # loss, accuracy\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            ###### 1 step 처리.\n",
    "            # 1. X,y를 device로 이동\n",
    "            X,y = X.to(device), y.to(device) \n",
    "            # 2. 추론\n",
    "            pred_proba = model(X)                     # 양성일 확률\n",
    "            pred_label = (pred_proba > 0.5).type(torch.int32) # >>> pred_proba의 값이 0.5보다 큰 것들은 true, 작으면 false. 이걸 int로 바꾸면 1,0 으로 바뀐다.\n",
    "            total_loss += loss_fn(pred_proba, y).item()\n",
    "            total_acc += (pred_label == y).sum().item() # >>> 예측한 것이 y와 같은지 확인하면 T/F로 바뀌고, 그걸 합치면 ture의 개수가 나오며 이것이 정답을 맞춘 개수가 된다.\n",
    "\n",
    "        # return loss acc\n",
    "        return total_loss / len(dataloader), total_acc / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8853d0-b137-47bb-8f0d-fc4f05700cf2",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd806dda-5058-4c44-a3f4-28cadc8a90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "EPOCHS = 3\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "816e18c2-17bf-4621-b630-b47e16c34bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/3] train loss: 0.5798266239391144, val loss: 0.4566967289923402, val acc: 0.78334\n",
      "[1/3] train loss: 0.42041753645234264, val loss: 0.40921463908822947, val acc: 0.81072\n",
      "[2/3] train loss: 0.38301309498008684, val loss: 0.39265494723149275, val acc: 0.81976\n",
      "930.4577136039734\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model, train_loader, loss_fn, optimizer, device)\n",
    "    val_loss, val_acc = test(model, test_loader, loss_fn, device)\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    print(f\"[{epoch}/{EPOCHS}] train loss: {train_loss}, val loss: {val_loss}, val acc: {val_acc}\")\n",
    "    \n",
    "e = time.time()\n",
    "\n",
    "print(e-s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b6ec21-1d3c-48c2-b7c3-314daea6576c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32690441-482a-46b1-b91b-b85329d2141f",
   "metadata": {},
   "source": [
    "## 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c16618-1517-4371-8e37-ae3cf03428b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"saved_models/nsmc/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d41e8-0715-4f50-aa37-11a8e142706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드할땐?\n",
    "# load_model = torch.load(\"saved_models/nsmc/model.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de7ed5-f7f6-4206-b16f-f8535a03405c",
   "metadata": {},
   "source": [
    "# 서비스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bdaa3-008d-4a93-aee6-0877e829ef32",
   "metadata": {},
   "source": [
    "## 전처리 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a2661a9-3964-4117-b273-e5d8bd4194b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "morph_tokenizer = Okt()\n",
    "def text_preprocessing(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]+\", ' ', text)\n",
    "    return ' '.join(morph_tokenizer.morphs(text, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "315603df-159b-4317-9fb4-7897546b7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_token_sequences(token_sequences, max_length):\n",
    "    \"\"\"padding 처리 메소드.\"\"\"\n",
    "    pad_token = tokenizer.token_to_id('[PAD]')  \n",
    "    seq_length = len(token_sequences)           \n",
    "    result = None\n",
    "    if seq_length > max_length:                 \n",
    "        result = token_sequences[:max_length]\n",
    "    else:                                            \n",
    "        result = token_sequences + ([pad_token] * (max_length - seq_length))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d73070a-0ee5-4f35-996c-b0d11ba08516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data_preprocessing(text_list):\n",
    "    \"\"\"\n",
    "    모델에 입력할 수 있는 input data를 생성\n",
    "    Args:\n",
    "        text_list: list - 추론할 댓글리스트\n",
    "    Returns\n",
    "        torch.LongTensor - 댓글 token_id tensor\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    text_list = [text_preprocessing(txt) for txt in text_list] # cleansing + 정규화\n",
    "    token_list = [tokenizer.encode(txt).ids for txt in text_list] # 정규화된 text를 토큰화\n",
    "    token_list = [pad_token_sequences(token, MAX_LENGTH) for token in token_list] # 토큰리스트의 크기(size)를 max_length에 맞추기\n",
    "\n",
    "    return torch.tensor(token_list, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e19997-6b61-446f-ac72-376cd34ee495",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6fb00ab-60d0-45f2-bb16-505a5f5cc056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 30])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_list = [\"아 진짜 재미없다.\", \"여기 식당 먹을만 해요\", \"이걸 영화라고 만들었냐?\", \"기대 안하고 봐서 그런지 괜찮은데.\", \"이걸 영화라고 만들었나?\", \"아! 뭐야 진짜.\", \"재미있는데.\", \"연기 짱 좋아. 한번 더 볼 의향도 있다.\", \"뭐 그럭저럭\"]\n",
    "input_tensor = predict_data_preprocessing(comment_list) # commnt_list를 토큰화해서 바로 넣어주게 해준다.\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "499c330d-69ff-43fb-9ee9-ad1c6054f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, comment_list:list[str], input_tensor:torch.tensor, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    model로 input_tensor를 추론해서 긍정/부정적인 댓글인지 출력\n",
    "    출력 형식\n",
    "        | comment(댓글) | label | 확률 |\n",
    "        | \"아 재미없다.\" |  부정  | 0.9 |   >>> 부정적인 댓글일 확률\n",
    "        | \"재밌다.\"     |  긍정  | 0.87|   >>> 긍정적인 댓글일 확률\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(input_tensor) # shape: batch, 1) -> pos일 확률\n",
    "        for txt, pos_proba in zip(comment_list, pred):\n",
    "            label = \"긍정적\" if pos_proba.item() > 0.5 else \"부정적\"\n",
    "            proba = pos_proba.item() if pos_proba.item() > 0.5 else 1-pos_proba.item() # 확률\n",
    "            # >>> 0.5 넘으면 긍정이므로 그냥 출력. 0.5 보다 작으면 부정이므로 1에서 빼서 부정일 확률로 표시\n",
    "            print(txt, label, round(proba, 3), sep=\"\\t\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5273ac02-81f3-4391-b802-f9dca1f8d032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 진짜 재미없다.\t긍정적\t0.535\n",
      "여기 식당 먹을만 해요\t부정적\t0.546\n",
      "이걸 영화라고 만들었냐?\t부정적\t0.927\n",
      "기대 안하고 봐서 그런지 괜찮은데.\t긍정적\t0.803\n",
      "이걸 영화라고 만들었나?\t부정적\t0.927\n",
      "아! 뭐야 진짜.\t부정적\t0.534\n",
      "재미있는데.\t부정적\t0.609\n",
      "연기 짱 좋아. 한번 더 볼 의향도 있다.\t부정적\t0.631\n",
      "뭐 그럭저럭\t부정적\t0.711\n"
     ]
    }
   ],
   "source": [
    "predict(model, comment_list, input_tensor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0510a47-9189-4c2a-a6e9-33b04971f2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석하려는 댓글을 입력하세요. 종료하려면 '!quit'을 입력하세요.\n",
      "돈 아깝다.\t부정적\t0.913\n",
      "볼만하다\t긍정적\t0.735\n",
      "노잼\t부정적\t0.527\n",
      "짜증난다\t긍정적\t0.728\n",
      "종료\n"
     ]
    }
   ],
   "source": [
    "print(\"분석하려는 댓글을 입력하세요. 종료하려면 '!quit'을 입력하세요.\")\n",
    "while True:\n",
    "    comment = input(\"댓글:\")\n",
    "    if comment == \"!quit\":\n",
    "        print(\"종료\")\n",
    "        break\n",
    "    comment_list = [comment]\n",
    "    input_tensor = predict_data_preprocessing([comment])\n",
    "    predict(model, comment_list, input_tensor, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
