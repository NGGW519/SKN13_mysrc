{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93c9500-6e1f-445a-a5e1-95e687853ae8",
   "metadata": {},
   "source": [
    "# Pytorch의 nn.Embedding\n",
    "- Pytorch의 Embedding Layer는 word2vec과 마찬가지로 word embedding vector를 찾는 **Lookup Table**이다.\n",
    "    - 단어의 **정수의 고유 index**가 입력으로 들어오면 Embedding Layer의 **그 index의 Vector**를 출력한다.\n",
    "    - 모델이 학습되는 동안 모델이 풀려는 문제에 맞는 값으로 Embedding Layer의 vector들이 업데이트 된다.\n",
    "    - Word2Vec의 embedding vector 학습을 nn.Embedding은 자신이 포함된 모델을 학습 하는 과정에서 한다고 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d936d8f7-a203-4452-978f-e2da81b7dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embed = nn.Embedding(\n",
    "    num_embeddings=20_000,  # vocab size(어휘사전에 있는 단어수(토큰수)) -> 총 몇개의 단어에 대한 embedding vector를 만들지에 대한 설정\n",
    "    embedding_dim=200,      # embedding vector의 차원수 -> 개별단어를 몇개의 숫자(feature)로 표현할지에 대한 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d166e4e-4ced-4bc8-a188-12358670ad0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20000, 200])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.weight\n",
    "# 아래 행렬을 보면, embed.weight[i]는 i번재 단어(token)의 embedding vector값을 나타낸다.\n",
    "# 따라서 20000만개의 embedding vector, 200개의 feature를 가짐\n",
    "# embed.weight.shape >>> 출력: torch.Size([20000, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c2253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 200])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding layer의 입력 - 문서를 구성하는 토큰들의 ID(**정수-int**)를 1차원 묶어서 전달.\n",
    "\n",
    "# 예를들어 doc = 나는-30|어제-159|밥을-9000|먹었다-326\n",
    "doc = torch.tensor([[30,159,9000,326],[30,159,9000,326],[30,159,9000,326]], dtype=torch.int64) # 각 토큰에 대한 id를 지정하여 아래처럼 정보 조회 가능\n",
    "                    # batch_size(문장 개수)가 3이라서 2차원리스트로 리스트 3개를 각각 넣어야한다.\n",
    "embedding_vector = embed(doc)\n",
    "embedding_vector.shape\n",
    "\n",
    "# 출력 결과: torch.Size([1, 4, 200]) >>>  [1:batch_size, 4:seq_len, 200:embedding vector 차원수]\n",
    "# 30, 159, 9000, 326 이라는 id가 200개의 차원을 갖는 vector로 바뀌는 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8210abaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2615, -0.6480,  0.6542,  0.1267, -0.4590, -1.5807, -0.4225,  2.0638,\n",
       "        -0.0388,  1.4628,  1.0835, -0.6870, -0.1393,  0.0141, -3.1681,  0.7286,\n",
       "         0.3485, -1.6117,  1.0816,  0.3718,  1.2983,  0.6510,  0.5335,  0.9103,\n",
       "        -0.6888,  0.5167, -0.6261, -0.3301, -0.3349,  0.1504, -0.6173, -0.3165,\n",
       "        -0.6558, -0.8535,  1.9076,  0.0057,  0.1901, -0.3131,  1.3714, -0.0869,\n",
       "         0.6757, -0.2383,  0.6748,  1.3897, -0.0878,  0.6294, -0.5268, -1.2177,\n",
       "         0.2837,  0.4953, -1.5418, -0.4043,  0.7954,  1.6025, -2.2578, -0.4963,\n",
       "        -0.8050,  0.5768, -0.9294, -1.0111, -0.1835,  1.0086, -0.5453, -0.4286,\n",
       "        -0.3882,  1.1476, -1.0053,  1.6031,  1.2461, -0.3062, -1.2062, -2.8213,\n",
       "        -1.1075, -0.5217,  0.2048, -0.1997,  1.7862,  0.1536,  1.6618,  0.1133,\n",
       "        -0.1169,  0.7271,  0.8280,  2.5958,  1.0975,  0.1266, -0.1898,  0.3918,\n",
       "         0.2049,  0.3505,  0.9007,  1.6800,  0.0518, -0.3274, -0.2015,  0.3683,\n",
       "        -0.1802, -0.1015,  0.0472,  1.4299,  2.1903, -0.8712,  0.0970, -0.9406,\n",
       "         0.8492,  1.5976,  0.5136,  1.5880,  0.6544,  0.3315,  1.4310, -0.7426,\n",
       "         2.0310, -0.0095,  0.6344, -1.2820, -0.1719, -0.1003,  0.1120,  0.5705,\n",
       "        -0.8667, -0.0484, -0.2008, -0.2347,  0.4048, -0.5157,  3.2819, -1.2849,\n",
       "         0.7035, -0.0582, -1.4262, -1.2471, -0.8687,  0.1329, -0.8047,  0.1590,\n",
       "        -0.6043,  0.8644, -0.8560, -0.4001, -0.2006, -1.2022,  1.7195,  0.1382,\n",
       "         0.2696,  0.3727, -0.5889, -1.9677, -0.6220,  0.1988, -0.1532, -1.3237,\n",
       "         0.2895, -0.1258, -1.3119,  0.9110,  0.1041, -0.8678,  0.7913,  1.6650,\n",
       "        -0.3066, -1.1589, -0.6972,  0.6780,  0.3401, -2.9907,  0.7362, -0.9732,\n",
       "        -1.4159, -1.1411,  1.0782,  0.6087, -0.3391,  2.0366,  1.0192,  0.6224,\n",
       "         1.0033, -1.1222,  0.0537,  0.1956, -0.4978,  0.7570,  1.4287, -1.2958,\n",
       "        -0.5413,  0.7787, -0.6163, -0.2096, -0.1444, -0.5403, -1.0728, -0.5669,\n",
       "        -1.4810, -0.5156,  0.4794, -0.1554, -0.4035, -0.6682, -0.0199,  0.9740],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8314e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2615, -0.6480,  0.6542,  0.1267, -0.4590, -1.5807, -0.4225,  2.0638,\n",
       "        -0.0388,  1.4628,  1.0835, -0.6870, -0.1393,  0.0141, -3.1681,  0.7286,\n",
       "         0.3485, -1.6117,  1.0816,  0.3718,  1.2983,  0.6510,  0.5335,  0.9103,\n",
       "        -0.6888,  0.5167, -0.6261, -0.3301, -0.3349,  0.1504, -0.6173, -0.3165,\n",
       "        -0.6558, -0.8535,  1.9076,  0.0057,  0.1901, -0.3131,  1.3714, -0.0869,\n",
       "         0.6757, -0.2383,  0.6748,  1.3897, -0.0878,  0.6294, -0.5268, -1.2177,\n",
       "         0.2837,  0.4953, -1.5418, -0.4043,  0.7954,  1.6025, -2.2578, -0.4963,\n",
       "        -0.8050,  0.5768, -0.9294, -1.0111, -0.1835,  1.0086, -0.5453, -0.4286,\n",
       "        -0.3882,  1.1476, -1.0053,  1.6031,  1.2461, -0.3062, -1.2062, -2.8213,\n",
       "        -1.1075, -0.5217,  0.2048, -0.1997,  1.7862,  0.1536,  1.6618,  0.1133,\n",
       "        -0.1169,  0.7271,  0.8280,  2.5958,  1.0975,  0.1266, -0.1898,  0.3918,\n",
       "         0.2049,  0.3505,  0.9007,  1.6800,  0.0518, -0.3274, -0.2015,  0.3683,\n",
       "        -0.1802, -0.1015,  0.0472,  1.4299,  2.1903, -0.8712,  0.0970, -0.9406,\n",
       "         0.8492,  1.5976,  0.5136,  1.5880,  0.6544,  0.3315,  1.4310, -0.7426,\n",
       "         2.0310, -0.0095,  0.6344, -1.2820, -0.1719, -0.1003,  0.1120,  0.5705,\n",
       "        -0.8667, -0.0484, -0.2008, -0.2347,  0.4048, -0.5157,  3.2819, -1.2849,\n",
       "         0.7035, -0.0582, -1.4262, -1.2471, -0.8687,  0.1329, -0.8047,  0.1590,\n",
       "        -0.6043,  0.8644, -0.8560, -0.4001, -0.2006, -1.2022,  1.7195,  0.1382,\n",
       "         0.2696,  0.3727, -0.5889, -1.9677, -0.6220,  0.1988, -0.1532, -1.3237,\n",
       "         0.2895, -0.1258, -1.3119,  0.9110,  0.1041, -0.8678,  0.7913,  1.6650,\n",
       "        -0.3066, -1.1589, -0.6972,  0.6780,  0.3401, -2.9907,  0.7362, -0.9732,\n",
       "        -1.4159, -1.1411,  1.0782,  0.6087, -0.3391,  2.0366,  1.0192,  0.6224,\n",
       "         1.0033, -1.1222,  0.0537,  0.1956, -0.4978,  0.7570,  1.4287, -1.2958,\n",
       "        -0.5413,  0.7787, -0.6163, -0.2096, -0.1444, -0.5403, -1.0728, -0.5669,\n",
       "        -1.4810, -0.5156,  0.4794, -0.1554, -0.4035, -0.6682, -0.0199,  0.9740],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.weight[30] # 위와 결과 같음\n",
    "# weight가 embedding_vector 값이다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302880db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72255e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca59e731-381d-4a79-83c4-1fc20ba006e1",
   "metadata": {},
   "source": [
    "# 네이버 영화 댓글 감성분석(Sentiment Analysis)\n",
    "\n",
    "## 감성분석(Sentiment Analysis) 이란\n",
    "입력된 텍스트가 **긍적적인 글**인지 **부정적인**인지 또는 **중립적인** 글인지 분석하는 것을 감성(감정) 분석이라고 한다.   \n",
    "이를 통해 기업이 고객이 자신들의 기업 또는 제품에 대해 어떤 의견을 가지고 있는지 분석한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7034ada-08b9-4163-b18d-ce429aef275b",
   "metadata": {},
   "source": [
    "# Dataset, DataLoader 생성\n",
    "\n",
    "## Korpora에서 Naver 영화 댓글 dataset 가져오기\n",
    "- https://ko-nlp.github.io/Korpora/ko-docs/corpuslist/nsmc.html\n",
    "- http://github.com/e9t/nsmc/\n",
    "    - input: 영화댓글\n",
    "    - output: 0(부정적댓글), 1(긍정적댓글)\n",
    "### API\n",
    "- **corpus 가져오기**\n",
    "    - `Korpora.load('nsmc')`\n",
    "- **text/label 조회**\n",
    "    - `corpus.get_all_texts()` : 전체 corpus의 text들을 tuple로 반환\n",
    "    - `corpus.get_all_labels()`: 전체 corpus의 label들을 list로 반환\n",
    "- **train/test set 나눠서 조회**\n",
    "    - `corpus.train`\n",
    "    - `corpus.test`\n",
    "    - `LabeledSentenceKorpusData` 객체에 text와 label들을 담아서 제공.\n",
    "        - `LabeledSentenceKorpusData.texts`: text들 tuple로 반환.\n",
    "        - `LabeledSentenceKorpusData.labels`: label들 list로 반환."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e2ea3-6123-4ebd-8e98-27b1db6406ed",
   "metadata": {},
   "source": [
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aebdb0ac-eaaa-47aa-a0de-11d49e8a427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:00, 75.0MB/s]                            \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 61.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from Korpora import Korpora\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b08d5-bfcd-4430-b4c0-44b19bbf4022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('아 더빙.. 진짜 짜증나네요 목소리',\n",
       " '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나',\n",
       " '너무재밓었다그래서보는것을추천한다',\n",
       " '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정',\n",
       " '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_inputs = corpus.get_all_texts() # inputs: 댓글들 전체\n",
    "all_labels = corpus.get_all_labels() # outputs: labels 전체 - 0: 부정, 1:긍정\n",
    "all_inputs[:5] #0~4번까지 댓글 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a23c0-06c5-49b7-b601-1ede1198b4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels[:5]\n",
    "\n",
    "# 출력결과: [0, 1, 0, 0, 1] >> 1번과 4번은 긍정적인 댓글로 파악한것\n",
    "# 위 데이터는 평점을 반영한 데이터라서, '너무재밓었다그래서보는것을추천한다' >> 긍정댓글인듯 보이지만, 평점을 낮게 줘서 '0'이라서 나온것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8899b607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NSMC.train: size=150000\n",
       "  - NSMC.train.texts : list[str]\n",
       "  - NSMC.train.labels : list[int]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e89634f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NSMC.test: size=50000\n",
       "  - NSMC.test.texts : list[str]\n",
       "  - NSMC.test.labels : list[int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9df89ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('굳 ㅋ',\n",
       " 'GDNTOPCLASSINTHECLUB',\n",
       " '뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아',\n",
       " '지루하지는 않은데 완전 막장임... 돈주고 보기에는....',\n",
       " '3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test.texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66b8e668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test.labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab492d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2e7e54a-548d-4bf3-81aa-357ab249f41a",
   "metadata": {},
   "source": [
    "## 토큰화\n",
    "1. 형태소 단위 token화(분절)를 먼저 한다.\n",
    "    - konlpy로 token화 한 뒤 다시 한 문장으로 만든다.\n",
    "2. 1에서 처리한 corpus를 BPE 로 token화\n",
    "   \n",
    "### 전처리 함수\n",
    "\n",
    "#### 형태소 단위 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0bbb39b-9f49-4d29-a969-4839c01f430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import string\n",
    "import re\n",
    "\n",
    "okt= Okt()\n",
    "# 전처리 = cleansing + 정규화(normalize)\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    1. 영문 -> 소문자로 변환\n",
    "    2. 구두점 제거\n",
    "    3. 형태소 기반 토큰화\n",
    "    4. 형태소로 토큰화 한 뒤 다시 하나의 문자열로 묶어서 반환.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # 구두점 제거(stop word(붕용어))\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
    "    # 정규화\n",
    "    tokens = okt.morphs(text, stem=True) # stem: 원형복원.\n",
    "    return ' '.join(tokens) # list인 [\"단어\", \"단어\"....] -> str로 변환 \"단어 단어 단어\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f45e2f-f76a-4011-b963-d7feb214cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "좀 검증된 애들좀 출현시켜라이탈리아 특집 장난하냐\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'좀 검증 되다 애 들 좀 출현 시키다 이탈리아 특집 장난 하다'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_inputs[400])\n",
    "text_preprocessing(all_inputs[400])   # 토큰화 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e658962-2fd6-4b1d-b4ef-a38de3ecc847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 걸린시간(초): 342.1757411956787\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "\n",
    "# train set 전처리\n",
    "train_texts = corpus.train.texts\n",
    "train_inputs = [text_preprocessing(txt) for txt in train_texts]\n",
    "train_labels = corpus.train.labels\n",
    "\n",
    "# test set 전처리\n",
    "test_texts = corpus.test.texts\n",
    "test_inputs = [text_preprocessing(txt) for txt in test_texts]\n",
    "test_labels = corpus.test.labels\n",
    "e = time.time()\n",
    "\n",
    "print('전처리 걸린시간(초):', e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18a25c93-7e26-4512-a0c0-56218fcda100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "os.makedirs(\"datasets/nsmc\", exist_ok=True)\n",
    "with open(\"datasets/nsmc/preprocessing_trainset.pkl\", \"wb\") as fw:\n",
    "    pickle.dump({\"input\":train_inputs, \"output\":train_labels}, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07c7cad7-23d9-4212-b07a-0e5da2ff73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/nsmc/preprocessing_testset.pkl\", \"wb\") as fw:\n",
    "    pickle.dump({\"input\":test_inputs, \"output\":test_labels}, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cc935fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = train_inputs + test_inputs # vocab 만들때 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b75a7213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000, 200000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs), len(test_inputs), len(all_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기까지는 전처리, 이하 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e519a68-d3a0-4481-bcf7-b121d8ba813f",
   "metadata": {},
   "source": [
    "### 토큰화\n",
    "- Subword 방식 토큰화 적용\n",
    "- Byte Pair Encoding 방식으로 huggingface tokenizer 사용\n",
    "    - BPE: 토큰을 글자 단위로 나눈뒤 가장 자주 등장하는 글자 쌍(byte paire)를 찾아 합친뒤 어휘사전에 추가한다.\n",
    "    - https://huggingface.co/docs/tokenizers/quicktour\n",
    "    - `pip install tokenizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f162bdf-fac9-4468-a264-c656e4b3164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE #, Unigram, WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocabs_size = 30_000  # 어휘사전의 최대 단어수\n",
    "min_frequency = 5 # 어휘를 다 넣지 않고, 최소 5번은 나와야 넣겠다는 것\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    BPE(unk_token=\"[UNK]\")\n",
    ")\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocabs_size,\n",
    "    min_frequency=min_frequency,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\"],\n",
    "    continuing_subword_prefix=\"##\"\n",
    "    # 단어 중간에 나오는 subword일 경우 앞에 ##을 붙인다.\n",
    "    # \"시작하는\" -> \"시작\", \"하는\" => \"시작\", \"##하는\"\n",
    ")\n",
    "tokenizer.train_from_iterator(all_inputs, trainer=trainer) #vocab 생성 == tokenizer 학습.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de9b29e1-384a-44e8-ab19-e53f0c1303c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26742"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 vocab size:\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998d38b-e762-4fd5-b37f-8f8a2b6f5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "os.makedirs(\"saved_models/nsmc\", exist_ok=True)\n",
    "tokenizer.save(\"saved_models/nsmc/tokenizer_bpe.json\")\n",
    "\n",
    "# 불러올땐?\n",
    "# load_tokenizer = Tokenizer.from_file(\"saved_models/nsmc/tokenizer_bpe.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81597ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정말 최고 의 명작 성인 이 되다 보다 이집트 의 왕자 는 또 다른 감동 그 자체 네 요\n",
      "[5420, 5438, 2203, 5530, 6570, 2206, 5425, 5410, 14757, 2203, 9123, 923, 1152, 5617, 5450, 651, 5641, 856, 2128]\n",
      "['정말', '최고', '의', '명작', '성인', '이', '되다', '보다', '이집트', '의', '왕자', '는', '또', '다른', '감동', '그', '자체', '네', '요']\n"
     ]
    }
   ],
   "source": [
    "# 토큰화되어 id가 매칭되는 것 확인. 1000번은 예시를 든것\n",
    "\n",
    "idx = 1000\n",
    "print(all_inputs[idx])\n",
    "tokens = tokenizer.encode(all_inputs[idx])\n",
    "print(tokens.ids)\n",
    "print(tokens.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf797f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'정말 최고 의 명작 성인 이 되다 보다 이집트 의 왕자 는 또 다른 감동 그 자체 네 요'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5c31d-633c-4a31-8f66-dff2ecf8e86a",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2441f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch 사용자 정의 Dataset(Custom Dataset) 정의\n",
    "# 1. Dataset 상속\n",
    "# 2. __len__(self) : 총 데이터의 개수 반환\n",
    "# 3. __getitem__(self, index) : index의 x,y를 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff27280e-dfb7-4947-9192-777e6984286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_length, tokenizer):\n",
    "        \"\"\"\n",
    "        texts: list - 댓글 목록. 리스트에 댓글들을 담아서 받는다. [\"댓글\", \"댓글\", ...]\n",
    "        labels: list - 댓글 감정(labels의 긍/부정) 목록. \n",
    "        max_length: 개별 댓글의 token 개수. 모든 댓글의 토큰수를 max_length에 맞춘다. 댓글마다 길이가 달라서 sequence의 length(개수)를 맞춰주기 위함.\n",
    "        tokenizer: Tokenizer\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        self.texts = [self.__pad_token_sequences(tokenizer.encode(txt).ids) for txt in texts]\n",
    "        # self.texts: 입력댓글 - token is로 변환된 댓글(문서). 글자수는 max_length에 맞춤.\n",
    "        #             max_length 보다 적으면 [PAD] 추가, max_length보다 많으면 잘라낸다.\n",
    "        #             아래의 __pad_token_sequences()가 그 역할을 한다.\n",
    "\n",
    "    ############################################################################################\n",
    "    # id로 구성된 개별 문장 token list를 받아서 패딩 추가 [20, 2, 1] => [20, 2, 1, 0, 0, 0, ..]\n",
    "    # max_length에 토큰리스트의 개수를 맞춰주는 함수.\n",
    "    ############################################################################################\n",
    "    def __pad_token_sequences(self, token_sequences): # 함수명 앞에만 '__'붙인 경우, class 안에서만 쓴다는 것(은닉성). 외부에서 호출 불가.\n",
    "                                                      # 뒤에도 '__' 붙이면 앞에 붙인 은닉성이 해제됨\n",
    "        \"\"\"\n",
    "        id로 구성된 개별 문서(댓글)의 token_id list를 받아서 max_length 길이에 맞추는 메소드\n",
    "        max_length 보다 토큰수가 적으면 [PAD] 추가, 많으면 max_length 크기로 줄인다.\n",
    "            ex) if)max_length = 5, [PAD] token id = 0 이라면?\n",
    "            >>>>> [20, 2, 1] => [20, 2, 1, 0, 0, 0, ..]\n",
    "            >>>>> [20, 30, 40, 50, 60, 70, 80] -> [20, 30, 40, 50, 60]\n",
    "            \n",
    "        \"\"\"\n",
    "        pad_token_id = self.tokenizer.token_to_id(\"[PAD]\")\n",
    "        seq_len = len(token_sequences) # 입력받은 토큰 개수\n",
    "        result = None\n",
    "        if seq_len > self.max_length: # 잘라내기\n",
    "            result = token_sequences[:self.max_length]\n",
    "        else:\n",
    "            result = token_sequences + ([pad_token_id] * (self.max_length - seq_len))\n",
    "            #[PAD]는 리스트이므로 [PAD]에 숫자를 곱하면 그만큼 더해진다.\n",
    "        return result\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels) # 총 데이터개수를 반환.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        idx 번째 text와 label을 학습 가능한 type으로 변환해서 반환\n",
    "        Parameter\n",
    "            idx: int 조회할 index\n",
    "        Return\n",
    "            tuple: (torch.LongTensor, torch.FloatTensor) - 댓글 토큰_id 리스트, 정답 Label\n",
    "        \"\"\"\n",
    "        txt = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return (torch.tensor(txt, dtype=torch.int64), torch.tensor(label, dtype=torch.float32))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9515bb6f-703d-4277-b645-8869267f5297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 11, 10, 9, 22]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_length = 5라고 두었지만, 5는 적절한 값일까?\n",
    "# 댓글의 길이가 어떻게 분포되었는지도 모르는데, 5라고 임의로 적용한건 적절치 않다.\n",
    "# 따라서 전체 댓글들의 토큰 수를 확인해보자!\n",
    "# 모든 댓글의 토큰 수 조회\n",
    "all_input_length = [len(tokenizer.encode(txt)) for txt in all_inputs]\n",
    "all_input_length[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "161f63e1-4ba8-4ccc-8c62-422b1f30b431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 89)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.min(all_input_length), np.max(all_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee963d58-0008-4fa6-b426-e3e332591f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29., 41.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(all_input_length, q=[0.9, 0.95])\n",
    "# 전체 중 90%의 토큰수는 29 미만, 95%는 41개 미만\n",
    "# 95%까진 반영하고 싶으면 max_length = 41로 두면된다. 아래는 max_length = 30으로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eea5f00e-70fa-475b-8e2d-d06d120e3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGHT = 30\n",
    "trainset = NSMCDataset(train_inputs, train_labels, MAX_LENGHT, tokenizer)\n",
    "testset = NSMCDataset(test_inputs, test_labels, MAX_LENGHT, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "83bf0d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc7532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1986, 5881, 5426, 5667, 6087,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0]),\n",
       " tensor(0.))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0] # 30개 맞춘것 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db59e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "643ebe07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2343, 782)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5f038-b32c-4e4e-82c8-956c7cbe0c4d",
   "metadata": {},
   "source": [
    "# 모델링\n",
    "- Embedding Layer를 이용해 Word Embedding Vector를 추출한다.\n",
    "- LSTM을 이용해 Feature 추출\n",
    "- Linear + Sigmoid로 댓글 긍정일 확률 출력\n",
    "  \n",
    "![outline](figures/rnn/RNN_outline.png)\n",
    "\n",
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88afd6-5c8f-4ade-b930-86c9425e86e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9cbbd8-d8b4-4a51-ac7a-5765722f139e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af1a0171-371e-4cb5-9bd5-ad1e3c14480d",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830d2b5-d5ed-4b53-a442-bebc83077aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04a25a-c6dc-4833-883b-54972a16c171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe5b3d-8b2f-4164-9b97-29e6aadced5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d229e-9a99-4a28-9dfd-9582686ba052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bdd5885-8150-4529-ad3a-84931a8824c5",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a1bf6-d8eb-42d0-996e-f975e93888af",
   "metadata": {},
   "source": [
    "### Train/Test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46099bec-eee3-4cef-921b-ce9ee6cf0f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5eb3ea-78e8-4248-a8ce-d07a4c362d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de42da4-8991-4bcb-9ce9-18155459dec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c8853d0-b137-47bb-8f0d-fc4f05700cf2",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd806dda-5058-4c44-a3f4-28cadc8a90d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e18c2-17bf-4621-b630-b47e16c34bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b6ec21-1d3c-48c2-b7c3-314daea6576c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32690441-482a-46b1-b91b-b85329d2141f",
   "metadata": {},
   "source": [
    "## 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c16618-1517-4371-8e37-ae3cf03428b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d41e8-0715-4f50-aa37-11a8e142706a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3de7ed5-f7f6-4206-b16f-f8535a03405c",
   "metadata": {},
   "source": [
    "# 서비스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bdaa3-008d-4a93-aee6-0877e829ef32",
   "metadata": {},
   "source": [
    "## 전처리 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2661a9-3964-4117-b273-e5d8bd4194b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]+\", ' ', text)\n",
    "    return ' '.join(morph_tokenizer.morphs(text, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315603df-159b-4317-9fb4-7897546b7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_token_sequences(token_sequences, max_length):\n",
    "    \"\"\"padding 처리 메소드.\"\"\"\n",
    "    pad_token = tokenizer.token_to_id('[PAD]')  \n",
    "    seq_length = len(token_sequences)           \n",
    "    result = None\n",
    "    if seq_length > max_length:                 \n",
    "        result = token_sequences[:max_length]\n",
    "    else:                                            \n",
    "        result = token_sequences + ([pad_token] * (max_length - seq_length))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d73070a-0ee5-4f35-996c-b0d11ba08516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data_preprocessing(text_list):\n",
    "    \"\"\"\n",
    "    모델에 입력할 수있는 input data를 생성\n",
    "    Parameter:\n",
    "        text_list: list - 추론할 댓글리스트\n",
    "    Return\n",
    "        torch.LongTensor - 댓글 token_id tensor\n",
    "    \"\"\"\n",
    "   \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e19997-6b61-446f-ac72-376cd34ee495",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb00ab-60d0-45f2-bb16-505a5f5cc056",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_list = [\"아 진짜 재미없다.\", \"여기 식당 먹을만 해요\", \"이걸 영화라고 만들었냐?\", \"기대 안하고 봐서 그런지 괜찮은데.\", \"이걸 영화라고 만들었나?\", \"아! 뭐야 진짜.\", \"재미있는데.\", \"연기 짱 좋아. 한번 더 볼 의향도 있다.\", \"뭐 그럭저럭\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c330d-69ff-43fb-9ee9-ad1c6054f9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5273ac02-81f3-4391-b802-f9dca1f8d032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0510a47-9189-4c2a-a6e9-33b04971f2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
